# APS360-Group35
# Mask Detection Dataset
The project utilizes data from three significant datasets:

- MAFA Dataset
- WIDER FACE Dataset
- Kaggle Face Mask Detection Dataset

The training set includes 6,819 images, while the validation set comprises 1,989 images.

### Data Sources and Dataset Composition
- Training Set: 6,819 images (3,006 from MAFA, 3,114 from WIDER Face, 709 from Kaggle)
- Validation Set: 1,989 images (1,059 from MAFA, 780 from WIDER Face, 150 from Kaggle)

### XML File Annotations
Each image in these datasets is accompanied by an XML file. These files include details such as the image's file name, path, size, and bounding box coordinates for faces.

### Preprocessing and Augmentation Techniques
Several preprocessing steps are implemented to enhance the model's performance:
- Photometric Distortions
- Random Cropping
- Horizontal Flipping
- Standardizing Image Sizes
- Normalization using ImageNet values

## Data Splitting
The datasets are split as follows:
- Training: ~70%
- Validation: ~15%
- Testing: ~15%

The split is index-based due to varying face counts in images. Index 900 in the validation dataset marks the start of the test dataset.

## Acknowledgements
This project utilizes datasets from the following sources:
- MAFA Dataset: [MAFA Dataset](http://www.escience.cn/people/geshiming/mafa.html)
- WIDER FACE Dataset: [WIDER FACE Dataset](http://shuoyang1213.me/WIDERFACE/)
- Kaggle Face Mask Detection Dataset: [Kaggle Dataset](https://www.kaggle.com/datasets/andrewmvd/face-mask-detection/discussion)

# Architecture
## Model
Our model is based on the architecture of the Single Shot Detection convolutional network with several improvements. 
## Components
- Base Network: A VGG16 pre-trained on the ImageNet classification tasks is used here to extract basic features from the image.
- Added Convolution Layers: Two more convolution Layers with 3x3 kernel were added after VGG to capture the features of pictures further.
- Feature Fusion Module : This module combines three feature maps, specifically conv4_3 and conv7 from VGG, along with the feature map from the last added convolution layer. To combine feature maps with different scales, a 1x1 convolution needs to be applied to each of them to reduce its dimension.  Since the expected output dimension matches the size of conv4_3, which is 38x38, the remaining feature maps need to be resized to 38x38 using the interpolate operation. Finally, the feature fusion map is generated by concatenating the resized feature maps.
- Auxiliary Convolution Pyramid: 6 convolution layers stack on the top of the feature fusion module. With these conv layers, feature maps progressively decreasing in size are generated to capture hierarchical information from the input picture.
- Priors: also known as anchor boxes, are applied to different feature maps to make potential predictions. The size, ratio, and number of priors are illustrated in the table.
- Channel Attention Module: Sequence and Extraction module are used before prediction-making to enhance the feature representation by selectively highlighting important regions
- Prediction Convolution Layers: Convolution layers are used for generating predictions for each anchor box at every location on the feature maps created by the Auxiliary ConvolutionPyramid. Two convolution layers for one feature map are used to predict bounding box offset and classification scores respectively.

## Output
The output of this network includes:
- `cls_preds`: These are the predictions for the class labels of the detected objects.
- `bbox_preds`: These are the predictions for the bounding boxes that enclose the detected objects.
## Loss Function
- To train the model, a loss function is utilized that considers both the classification and bounding box predictions. In this case, the chosen loss functions are CrossEntropyLoss for the classification task and L1Loss (mean absolute error) for the bounding box prediction task. These loss functions enable the model to optimize both the accuracy of object classification and the precision of bounding box localization.
- SSD matches each anchor box to the ground truth objects with maximum overlapping. However, there could be multiple priors with max IOU, and the traditional SSD randomly selects one. In contrast, our model selects the most suitable prior based on a heuristic function that considers both the closest ratio and Euclidean distance.
## Perfomance
- Real-time detection if a person is wearing a mask or not
- Process uploaded images using the employed model and make the detection of whether a person is wearing a mask or not within 0.005 seconds. 
- MAP reaches 0.848 for test+validation Dataset


